{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 121
    },
    "colab_type": "code",
    "id": "93e-ZnpaE8R7",
    "outputId": "11801864-e69d-4d53-c675-e0390d6c19fa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
      "\n",
      "Enter your authorization code:\n",
      "4/3wF3MyfZxrxiRJSdVTcChXj88X7zAh_18dwYBlQyUruNgzRzyMoQyJE\n",
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JltjDykKE9DI"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import wave\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.optimize import minpack2\n",
    "import librosa\n",
    "import librosa.display\n",
    "import librosa.decompose\n",
    "import IPython.display as ipd\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa as lr\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uT3oNk3dFG-X",
    "outputId": "55cc2124-218a-4e92-811f-d3a7c30db51f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir = \"/content/drive/My Drive/ML Project/AUDIO\"\n",
    "#x , sr = librosa.load(data_dir)\n",
    "audio_files = glob(data_dir + '/*.wav')\n",
    "\n",
    "len(audio_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 185
    },
    "colab_type": "code",
    "id": "YWmWOzdKFMEa",
    "outputId": "9508bee7-61ad-401e-a201-348fe7c79b82"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyAudioAnalysis\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/a7/21c523c77c90215137809d4c006553e02318cdd4a847f8d18b4ae0d01e1b/pyAudioAnalysis-0.3.5.tar.gz (41.2MB)\n",
      "\u001b[K     |████████████████████████████████| 41.2MB 87kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: pyAudioAnalysis\n",
      "  Building wheel for pyAudioAnalysis (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for pyAudioAnalysis: filename=pyAudioAnalysis-0.3.5-cp36-none-any.whl size=41161777 sha256=e3cb04712495a7714ac8b5f3911e5dbe5a3b6445b57d35ab8a480eda215250ba\n",
      "  Stored in directory: /root/.cache/pip/wheels/7d/6f/10/fa7739a0f0de564fdf368cff6102060f845f1e734bc670d35d\n",
      "Successfully built pyAudioAnalysis\n",
      "Installing collected packages: pyAudioAnalysis\n",
      "Successfully installed pyAudioAnalysis-0.3.5\n"
     ]
    }
   ],
   "source": [
    "pip install pyAudioAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "colab_type": "code",
    "id": "_dnpLaZIFO5V",
    "outputId": "56d375ec-dede-4af4-8aa0-0c754507e910"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting eyeD3\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/67/202bcc28b01684f8fe18921f4b1dbe44b471b2c407f5d784849d513e417f/eyeD3-0.9.5-py2.py3-none-any.whl (145kB)\n",
      "\r",
      "\u001b[K     |██▎                             | 10kB 22.7MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 20kB 5.6MB/s eta 0:00:01\r",
      "\u001b[K     |██████▊                         | 30kB 7.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 40kB 7.6MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▎                    | 51kB 6.6MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 61kB 7.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▉                | 71kB 7.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 81kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▎           | 92kB 7.8MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▋         | 102kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▉       | 112kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 122kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▎  | 133kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▋| 143kB 8.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 153kB 8.1MB/s \n",
      "\u001b[?25hCollecting deprecation\n",
      "  Downloading https://files.pythonhosted.org/packages/02/c3/253a89ee03fc9b9682f1541728eb66db7db22148cd94f89ab22528cd1e1b/deprecation-2.1.0-py2.py3-none-any.whl\n",
      "Collecting filetype\n",
      "  Downloading https://files.pythonhosted.org/packages/b4/6b/7bc015da1a576ac037582ae0c5acb675371de9e017e860931e97a428ee31/filetype-1.0.7-py2.py3-none-any.whl\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from eyeD3) (0.7)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from deprecation->eyeD3) (20.4)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->deprecation->eyeD3) (1.15.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->deprecation->eyeD3) (2.4.7)\n",
      "Installing collected packages: deprecation, filetype, eyeD3\n",
      "Successfully installed deprecation-2.1.0 eyeD3-0.9.5 filetype-1.0.7\n"
     ]
    }
   ],
   "source": [
    "pip install eyeD3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "FcUGeFyRFQ5b",
    "outputId": "838810ad-b51d-4995-9595-87e696fac124"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hmmlearn\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ff/7b/33f629a443a0671161c019e55c3f1b511c7e9fdce5ab8c8c3c33470eb939/hmmlearn-0.2.3-cp36-cp36m-manylinux1_x86_64.whl (363kB)\n",
      "\r",
      "\u001b[K     |█                               | 10kB 18.4MB/s eta 0:00:01\r",
      "\u001b[K     |█▉                              | 20kB 14.1MB/s eta 0:00:01\r",
      "\u001b[K     |██▊                             | 30kB 7.1MB/s eta 0:00:01\r",
      "\u001b[K     |███▋                            | 40kB 8.5MB/s eta 0:00:01\r",
      "\u001b[K     |████▌                           | 51kB 7.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████▍                          | 61kB 8.3MB/s eta 0:00:01\r",
      "\u001b[K     |██████▎                         | 71kB 8.7MB/s eta 0:00:01\r",
      "\u001b[K     |███████▏                        | 81kB 9.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████                        | 92kB 9.3MB/s eta 0:00:01\r",
      "\u001b[K     |█████████                       | 102kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████                      | 112kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▉                     | 122kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 133kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▋                   | 143kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▌                  | 153kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▍                 | 163kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▎                | 174kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████▏               | 184kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 194kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████              | 204kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 215kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▉            | 225kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████▊           | 235kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▋          | 245kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████▌         | 256kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▍        | 266kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████▎       | 276kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▏      | 286kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████      | 296kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████     | 307kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████    | 317kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████▉   | 327kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▊  | 337kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████████████████████▋ | 348kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▌| 358kB 10.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 368kB 10.1MB/s \n",
      "\u001b[?25hRequirement already satisfied, skipping upgrade: scipy>=0.15 in /usr/local/lib/python3.6/dist-packages (from hmmlearn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.16 in /usr/local/lib/python3.6/dist-packages (from hmmlearn) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from hmmlearn) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.16->hmmlearn) (0.16.0)\n",
      "Installing collected packages: hmmlearn\n",
      "Successfully installed hmmlearn-0.2.3\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade --user hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "0mnK_EApFSi9",
    "outputId": "67c7a35b-cf48-475b-9382-3cdf3717674f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pydub\n",
      "  Downloading https://files.pythonhosted.org/packages/7b/d1/fbfa79371a8cd9bb15c2e3c480d7e6e340ed5cc55005174e16f48418333a/pydub-0.24.1-py2.py3-none-any.whl\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.24.1\n"
     ]
    }
   ],
   "source": [
    "pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 302
    },
    "colab_type": "code",
    "id": "cXfzfl6pFUQc",
    "outputId": "4edf2f12-1d54-46c7-9fa1-de2b36aa984a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/hmmlearn/hmmlearn\n",
      "  Cloning https://github.com/hmmlearn/hmmlearn to /tmp/pip-req-build-0y0w_03n\n",
      "  Running command git clone -q https://github.com/hmmlearn/hmmlearn /tmp/pip-req-build-0y0w_03n\n",
      "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from hmmlearn==0.2.3.post14+gb8a11e9) (1.18.5)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.6/dist-packages (from hmmlearn==0.2.3.post14+gb8a11e9) (0.22.2.post1)\n",
      "Requirement already satisfied: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from hmmlearn==0.2.3.post14+gb8a11e9) (1.4.1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.16->hmmlearn==0.2.3.post14+gb8a11e9) (0.16.0)\n",
      "Building wheels for collected packages: hmmlearn\n",
      "  Building wheel for hmmlearn (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for hmmlearn: filename=hmmlearn-0.2.3.post14+gb8a11e9-cp36-cp36m-linux_x86_64.whl size=324376 sha256=bacfe8d304ca356e2349fa08ecbd96478fb973bf2581969f995a371703befba1\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-t1_ks0ga/wheels/02/e9/26/ba89ec3c567c3583f0d34b8627dd594b651d41304a59eeb8f3\n",
      "Successfully built hmmlearn\n",
      "Installing collected packages: hmmlearn\n",
      "  Found existing installation: hmmlearn 0.2.3\n",
      "    Uninstalling hmmlearn-0.2.3:\n",
      "      Successfully uninstalled hmmlearn-0.2.3\n",
      "Successfully installed hmmlearn-0.2.3.post14+gb8a11e9\n"
     ]
    }
   ],
   "source": [
    "pip install --user git+https://github.com/hmmlearn/hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 101
    },
    "colab_type": "code",
    "id": "emau6MuqFV85",
    "outputId": "d64f3a9d-e002-4a86-a466-0fb7464ea6ed"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: hmmlearn in /root/.local/lib/python3.6/site-packages (0.2.3.post14+gb8a11e9)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19 in /usr/local/lib/python3.6/dist-packages (from hmmlearn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: scikit-learn>=0.16 in /usr/local/lib/python3.6/dist-packages (from hmmlearn) (0.22.2.post1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from hmmlearn) (1.18.5)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.16->hmmlearn) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "pip install -U --user hmmlearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 202
    },
    "colab_type": "code",
    "id": "nwmN-9OpFrEz",
    "outputId": "b17bb44f-ac16-4e89-b607-6b9376edc8d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hmmlearn==0.2.3\n",
      "  Using cached https://files.pythonhosted.org/packages/ff/7b/33f629a443a0671161c019e55c3f1b511c7e9fdce5ab8c8c3c33470eb939/hmmlearn-0.2.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: scipy>=0.15 in /usr/local/lib/python3.6/dist-packages (from hmmlearn==0.2.3) (1.4.1)\n",
      "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from hmmlearn==0.2.3) (1.18.5)\n",
      "Requirement already satisfied: scikit-learn>=0.16 in /usr/local/lib/python3.6/dist-packages (from hmmlearn==0.2.3) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.16->hmmlearn==0.2.3) (0.16.0)\n",
      "Installing collected packages: hmmlearn\n",
      "  Found existing installation: hmmlearn 0.2.3.post14+gb8a11e9\n",
      "    Uninstalling hmmlearn-0.2.3.post14+gb8a11e9:\n",
      "      Successfully uninstalled hmmlearn-0.2.3.post14+gb8a11e9\n",
      "Successfully installed hmmlearn-0.2.3\n"
     ]
    }
   ],
   "source": [
    "pip install \"hmmlearn==0.2.3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Cp6igPt7Q_jd",
    "outputId": "651f2c7c-455c-49c1-db78-737db0a207b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyAudioAnalysis in /usr/local/lib/python3.6/dist-packages (0.3.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyAudioAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 120
    },
    "colab_type": "code",
    "id": "HK62q6CNFYc7",
    "outputId": "e39e3dcc-1396-48d8-b546-3cabbdd8e336"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'import os\\nimport pyAudioAnalysis\\nfrom pyAudioAnalysis import audioBasicIO as aIO\\nfrom pyAudioAnalysis import audioSegmentation as aS\\nimport scipy.io.wavfile as wavfile\\nimport wave\\nimport pyAudioAnalysis.audioBasicIO\\nfrom pyAudioAnalysis.audioBasicIO import read_audio_file\\nfrom pyAudioAnalysis.audioSegmentation import silence_removal\\n\\n\\n#we have used pyAudio but pyDub also can be used for this , but due to less documentaion and refrence module we selected pyAudio.\\n#link for research paper\\n#https://www.researchgate.net/publication/286637817_pyAudioAnalysis_An_Open-Source_Python_Library_for_Audio_Signal_Analysis/figures?lo=1\\n\\n\\n\\n\"\"\"\\nA script that iterates through the extracted wav files and uses\\npyAudioAnalysis\\' silence extraction module to make a wav file containing the\\nsegmented audio (when the participant is speaking -- silence and virtual\\ninterviewer speech removed)\\n\"\"\"\\n\\n\\ndef remove_silence(filename, out_dir, smoothing=1.0, weight=0.3, plot=False):\\n    \"\"\"\\n    A function that implements pyAudioAnalysis\\' silence extraction module\\n    and creates wav files of the participant specific portions of audio. The\\n     AVEC 2016 dataset was taken as refrence.\\n    Parameters\\n    ----------\\n    filename : filepath\\n        path to the input wav file\\n    out_dir : filepath\\n        path to the desired directory (where a participant folder will\\n        be created containing a \\'PXXX_number.wav\\' file)\\n    smoothing : float\\n        tunable parameter to compensate for sparseness of recordings\\n    weight : float\\n        probability threshold for silence removal used in SVM\\n        Returns\\n    -------\\n    A folder for each participant containing a single wav file\\n    (named \\'PXXX_number.wav\\') with the majority of silence\\n    and virtual interviewer speech removed. Feature extraction is\\n    performed on these segmented wav files.\\n    \"\"\"\\n    partic_id = \\'P\\' + filename.split(\\'/\\')[-1].split(\\'_\\')[0]  # PXXX\\n    \\n        # create participant directory for segmented wav files\\n       participant_dir = os.path.join(out_dir, partic_id)\\n       if not os.path.exists(participant_dir):\\n        os.makedirs(participant_dir)\\n\\n        os.chdir(participant_dir)\\n\\n        [Fs, x] = read_audio_file(filename)\\n        segments = silence_removal(x, Fs)\\n\\n        for s in segments:\\n          seg_name = \"{:s}_{:.2f}-{:.2f}.wav\".format(partic_id, s[0], s[1])\\n          wavfile.write(seg_name, Fs, x[int(Fs * s[0]):int(Fs * s[1])])\\n\\n        # concatenate segmented wave files within participant directory\\n       concatenate_segments(participant_dir, partic_id)\\n\\n\\n\\n\\n     # AGAIN AVEC 2016 dataset was taken as refrence.\\n\\ndef concatenate_segments(participant_dir, partic_id, remove_segment=True):\\n    \"\"\"\\n    A function that concatenates all the wave files in a participants\\n    directory in to single wav file (with silence and other speakers removed)\\n    and writes in to the participant\\'s directory, then removes the individual\\n    segments (when remove_segment=True).\\n    \"\"\"\\n    infiles = os.listdir(participant_dir)  # list of wav files in directory\\n    outfile = \\'{}_no_silence.wav\\'.format(partic_id)\\n\\n    data = []\\n    for infile in infiles:\\n        w = wave.open(infile, \\'rb\\')\\n        #RB:Read only mode. \\n        data.append([w.getparams(), w.readframes(w.getnframes())])\\n        w.close()\\n        if remove_segment:\\n            os.remove(infile)\\n\\n    output = wave.open(outfile, \\'wb\\')\\n    #WB:Write only mode.\\n    # details of the files must be the same\\n    output.setparams(data[0][0])\\n    #setparams:Sets all parameters. Like frames channels numberof_frames\\n    # write each segment to output\\n    for idx in range(len(data)):\\n        output.writeframes(data[idx][1])\\n    output.close()\\n\\n\\nif __name__ == \\'__main__\\':\\n    # directory containing raw wav files\\n    dir_name = \\'/content/drive/My Drive/ML Project/AUDIO\\'\\n\\n    # directory where a participant folder will be created containing their\\n    # segmented wav file\\n    out_dir = \\'/content/drive/My Drive/ML Project/SPLIT AUDIO\\'\\n\\n    # iterate through wav files in dir_name and create a segmented wav_file\\n    for file in os.listdir(dir_name):\\n        if file.endswith(\\'.wav\\'):\\n            filename = os.path.join(dir_name, file)\\n            remove_silence(filename, out_dir)\\n            '"
      ]
     },
     "execution_count": 2,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import os\n",
    "import pyAudioAnalysis\n",
    "from pyAudioAnalysis import audioBasicIO as aIO\n",
    "from pyAudioAnalysis import audioSegmentation as aS\n",
    "import scipy.io.wavfile as wavfile\n",
    "import wave\n",
    "import pyAudioAnalysis.audioBasicIO\n",
    "from pyAudioAnalysis.audioBasicIO import read_audio_file\n",
    "from pyAudioAnalysis.audioSegmentation import silence_removal\n",
    "\n",
    "\n",
    "#we have used pyAudio but pyDub also can be used for this , but due to less documentaion and refrence module we selected pyAudio.\n",
    "#link for research paper\n",
    "#https://www.researchgate.net/publication/286637817_pyAudioAnalysis_An_Open-Source_Python_Library_for_Audio_Signal_Analysis/figures?lo=1\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "A script that iterates through the extracted wav files and uses\n",
    "pyAudioAnalysis' silence extraction module to make a wav file containing the\n",
    "segmented audio (when the participant is speaking -- silence and virtual\n",
    "interviewer speech removed)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def remove_silence(filename, out_dir, smoothing=1.0, weight=0.3):\n",
    "    \"\"\"\n",
    "    A function that implements pyAudioAnalysis' silence extraction module\n",
    "    and creates wav files of the participant specific portions of audio. The\n",
    "     AVEC 2016 dataset was taken as refrence.\n",
    "    Parameters\n",
    "    \n",
    "    filename : filepath\n",
    "        path to the input wav file\n",
    "    out_dir : filepath\n",
    "        path to the desired directory (where a participant folder will\n",
    "        be created containing a 'PXXX_number.wav' file)\n",
    "    smoothing : float\n",
    "        tunable parameter to compensate for sparseness of recordings\n",
    "    weight : float\n",
    "        probability threshold for silence removal used in SVM\n",
    "        Returns\n",
    "  \n",
    "    A folder for each participant containing a single wav file\n",
    "    (named 'PXXX_number.wav') with the majority of silence\n",
    "    and virtual interviewer speech removed. Feature extraction is\n",
    "    performed on these segmented wav files.\n",
    "    \"\"\"\n",
    "    partic_id = 'P' + filename.split('/')[-1].split('_')[0]  # PXXX\n",
    "    \n",
    "        # create participant directory for segmented wav files\n",
    "       participant_dir = os.path.join(out_dir, partic_id)\n",
    "       if not os.path.exists(participant_dir):\n",
    "        os.makedirs(participant_dir)\n",
    "\n",
    "        os.chdir(participant_dir)\n",
    "\n",
    "        [Fs, x] = read_audio_file(filename)\n",
    "        segments = silence_removal(x, Fs)\n",
    "\n",
    "        for s in segments:\n",
    "          seg_name = \"{:s}_{:.2f}-{:.2f}.wav\".format(partic_id, s[0], s[1])\n",
    "          wavfile.write(seg_name, Fs, x[int(Fs * s[0]):int(Fs * s[1])])\n",
    "\n",
    "        # concatenate segmented wave files within participant directory\n",
    "       concatenate_segments(participant_dir, partic_id)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "     # AGAIN AVEC 2016 dataset was taken as refrence.\n",
    "\n",
    "def concatenate_segments(participant_dir, partic_id, remove_segment=True):\n",
    "    \"\"\"\n",
    "    A function that concatenates all the wave files in a participants\n",
    "    directory in to single wav file (with silence and other speakers removed)\n",
    "    and writes in to the participant's directory, then removes the individual\n",
    "    segments (when remove_segment=True).\n",
    "    \"\"\"\n",
    "    infiles = os.listdir(participant_dir)  # list of wav files in directory\n",
    "    outfile = '{}_no_silence.wav'.format(partic_id)\n",
    "\n",
    "    data = []\n",
    "    for infile in infiles:\n",
    "        w = wave.open(infile, 'rb')\n",
    "        #RB:Read only mode. \n",
    "        data.append([w.getparams(), w.readframes(w.getnframes())])\n",
    "        w.close()\n",
    "        if remove_segment:\n",
    "            os.remove(infile)\n",
    "\n",
    "    output = wave.open(outfile, 'wb')\n",
    "    #WB:Write only mode.\n",
    "    # details of the files must be the same\n",
    "    output.setparams(data[0][0])\n",
    "    #setparams:Sets all parameters. Like frames channels numberof_frames\n",
    "    # write each segment to output\n",
    "    for idx in range(len(data)):\n",
    "        output.writeframes(data[idx][1])\n",
    "    output.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # directory containing raw wav files\n",
    "    dir_name = '/content/drive/My Drive/ML Project/AUDIO'\n",
    "\n",
    "    # directory where a participant folder will be created containing their\n",
    "    # segmented wav file\n",
    "    out_dir = '/content/drive/My Drive/ML Project/SPLIT AUDIO'\n",
    "\n",
    "    # iterate through wav files in dir_name and create a segmented wav_file\n",
    "    for file in os.listdir(dir_name):\n",
    "        if file.endswith('.wav'):\n",
    "            filename = os.path.join(dir_name, file)\n",
    "            remove_silence(filename, out_dir)\n",
    "            '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gLjsNvoGRsJ6"
   },
   "outputs": [],
   "source": [
    "#audio_directory = '/content/drive/My Drive/ML Project/AUDIO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qpzg4wqrSBL1"
   },
   "outputs": [],
   "source": [
    "#for file in os.listdir(audio_directory):\n",
    " # remove_silence(audio_directory + file, '/content/drive/My Drive/ML Project/SPLIT AUDIO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xbKTkw30Sbcp"
   },
   "outputs": [],
   "source": [
    "#from pyAudioAnalysis.audioSegmentation import silence_removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MsgHpLxVTDUf"
   },
   "outputs": [],
   "source": [
    "#f1='/content/drive/My Drive/ML Project/AUDIO/300_AUDIO.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2Fw-7KTKWIPH"
   },
   "outputs": [],
   "source": [
    "#f2='/content/drive/My Drive/ML Project/SPLIT AUDIO'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hh3Na-N0WOPi"
   },
   "outputs": [],
   "source": [
    "#remove_silence(f1,f2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSAjvwqKzwth"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "audio-split-demo",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
